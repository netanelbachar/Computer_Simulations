#!/local/hirshb/anaconda2/bin/python
import numpy as np
import pandas as pd
def PlotWStyle(x, y, Nrow=1, Ncol=1, Ncurrent=1, xlabel='', ylabel='', title=''):
    #THIS FUNCTION ADDS SUBPLOTS TO FIGURE USING 2 DATA VECTORS, X AND Y.
    #NROW/NCOLS = NUMBER OF ROWS/COLUMNS OF SUBPLOTS. DEFAULT = 1 AND 1.
    #NCURRENT IS THE CURRENT POSITION OF THE SUBPLOT. DEFAULT IS 1.
    #XLABEL AND YLABEL - LABELS FOR AXES. DEFAULT IS NOT LABEL.
    #A SCIENTIFIC FORMAT AND GRID ARE ENFORCED ON Y AXES.
    #MEAN VALUES FOR EACH PLOT IS ADDED AS HORIZONTAL RED LINE AND TITLE FOR EACH 
    #SUBPLOT PRESENTS THE MEAN +- STD.
    
    import matplotlib.pyplot as plt
    plt.subplot(Nrow,Ncol,Ncurrent)
    
    #FORCE SCIENTIFIC FORMAT
    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))
    plt.plot(x,y)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    
    #FORCE GRID
    plt.grid(True)
    
    #ADD HORIZONTAL LINE FOR MEAN
    plt.axhline(y=y.mean(), color='r', linestyle='-')
    
    #FORCE TITLE TO BE MEAN +- STD
    plt.title(title)

def readXYZofBead(fname, Natoms):
    #READS XYZ FILES FNAME.xyz 
    import numpy as np

    print('Reading XYZ file '+ fname)
    try:
        #READ XYZ FILES STEP by STEP
        #SAVES THEM IN 3 ARRAYS X_ALL, Y_ALL AND Z_ALL. 
        #INDIVIDUAL ATOMS AT SPECIFIC TIME ARE ACCESSED BY X_ALL[STEP][ATOM]
        f = open(fname)
        x_all=[]
        y_all=[]
        z_all=[]
        line = f.readline()
        
        while (line):
            line = f.readline()
            x=[]
            y=[]
            z=[]
            for atom in range(Natoms):
                line = f.readline().split()
                x.append(float(line[1]))
                y.append(float(line[2]))
                z.append(float(line[3]))
            x_all.append(x)
            y_all.append(y)
            z_all.append(z)
            line = f.readline()
    except:     
        raise IOError('Could not find ' + fname)

    #print(x_all)
    xarr = np.array(x_all)
    yarr = np.array(y_all)
    zarr = np.array(z_all)
    return [xarr ,yarr ,zarr]

def ReadXYZofAllBeads(Nbeads, path, Natoms, prefixdir="/P", prefixfile="/data.pos_", shift=0, ifscale2Bohr=True):
    #THIS FUNCTIONS READS COORDINATES OF ALL BEADS BY CALLIN readXYZforBead for each BEADS.
    #PREFIXDIR, PREFIXFILE AND SHIFT HELP FIND THE FILES WHICH HAVE A SYSTEMATIC FILENAME.
    from beads import Beads
    from auxfunctions import readXYZofBead
    
    Ang2Bohr = 1.889725989
    
    #Read  coordinate of all beads
    allbeads=[]
    for bead in range(Nbeads):
    
        #Correct for 0s in fname based on Nbeads
        if (Nbeads >= 10 and Nbeads <= 99):
            if (bead+shift < 10):
                #fname = path + prefixdir + str(Nbeads) + "/" + prefixfile + "0"  + str(bead+shift) + ".xyz"
                fname = path + "/" + prefixfile + "0"  + str(bead+shift) + ".xyz"
            else:
                #fname = path + prefixdir + str(Nbeads) + "/"+ prefixfile  + str(bead+shift) + ".xyz"
                fname = path + "/"+ prefixfile  + str(bead+shift) + ".xyz"
        elif (Nbeads > 99):
            raise IOError("Nbeads is larger than 99!")
        else:
            #fname = path + prefixdir + str(Nbeads) + "/" + prefixfile + str(bead+shift) + ".xyz"
            fname = path + "/" + prefixfile + str(bead+shift) + ".xyz"
        
        #READ XYZ OF ALL ATOMS
        [x,y,z] = readXYZofBead(fname,Natoms)
        myBead = Beads()
        if ifscale2Bohr:
            myBead.x = x *  Ang2Bohr
            myBead.y = y *  Ang2Bohr
            myBead.z = z *  Ang2Bohr
        else:
            myBead.x = x 
            myBead.y = y 
            myBead.z = z
        allbeads.append(myBead)
    return allbeads

def readPLUMEDfes(fname,Nskip):
    #READS PLUMED FILE FNAME AND SKIPS NSKIP LINES
    import pandas as pd 

    print('Reading PLUMED file '+ fname)
    print('Note: we assume structure is CV1, CV2, FES,...')
    try:
        #READ WHOLE CSV FILE GENERATED BY PLUMED
        #IGNORE FIRST NSKIP ROWS, DELIMETER IS ASSUMED TO BE ONE OR MORE SPACES
        data = pd.read_csv(fname, sep='\s+', header=None, skiprows=Nskip)

    except:     
        raise IOError('Could not find ' + fname)

    return data

def Parse_args_heatmap():
    #Defining and parsing argumens for heatmap
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("fname", type=str,
                        help="PLUMED file name")
    parser.add_argument("NCVs", type=int, choices=[1,2], 
                        help="Number of CVs")
    parser.add_argument("Nskip", type=int, 
                        help="Number of lines to skip")
    parser.add_argument("--CV1", type=str, default='CV1',
                        help="Label for CV1 as 'label1' ")
    parser.add_argument("--CV2", type=str, default='CV2',
                        help="Label for CV2 as 'label2' ")

    args = parser.parse_args()
    return args

def GetHistForBlockOneSym(s,  ind_0, ind_l,  bin_range, Nbins, beta, allbeads, sign, dim):
    import numpy as np
    import pandas as pd
    
    Nbeads = len(allbeads)
    W = (1 + sign * np.exp(-beta * s))
    
    save = np.zeros(Nbins)
    s = np.array(s)
    for counter, step in enumerate(range(ind_0, ind_l)):
        save_c = np.zeros(Nbins)
        for bead in range(Nbeads):
            dist = allbeads[bead].calc_dist(step, dim)
            (c, b) = np.histogram(dist, range=bin_range, bins=Nbins)
            save_c = save_c + c
        save_c = save_c/Nbeads
        
        save = save + save_c*float(W.iloc[counter])
    
    points = 0.5*(b[:-1]+b[1:])
    return save, points

def GetHistForBlockAllSym(s,  ind_0, ind_l,  bin_range, Nbins, beta, allbeads, sign, dim):
    import numpy as np
    import pandas as pd
    
    W={}
    save={}
    Nbeads = len(allbeads)
    s = np.array(s)
    for value in sign:
        
        W[value] = (1 + value * np.exp(-beta * s))
        save[value] = np.zeros(Nbins)
        
    
    for counter, step in enumerate(range(ind_0, ind_l)):
        save_c = np.zeros(Nbins)
        for bead in range(Nbeads):
            dist = allbeads[bead].calc_dist(step, dim)
            (c, b) = np.histogram(dist, range=bin_range, bins=Nbins)
            save_c = save_c + c
        save_c = save_c/Nbeads
        for value in sign:
            save[value] = save[value] + save_c*float(W[value][counter])
        
    points = 0.5*(b[:-1]+b[1:])
    return save, points

def GetHistForBlockAllSymPLUMED(s, data_dist,  ind_0, ind_l,  bin_range, Nbins, beta, Nbeads, sign, dim, weights = None):
    import numpy as np
    import pandas as pd
    
    W={}
    save={}
    s = np.array(s)
    for value in sign:
        
        W[value] = (1 + value * np.exp(-beta * s))
        save[value] = np.zeros(Nbins)
        
        for bead in range(Nbeads):
            if weights is not None:
                (c,b) = np.histogram(data_dist[bead], range=bin_range, bins=Nbins, weights=pd.DataFrame(W[value]*weights))
                save[value] += c
            else:
                (c,b) = np.histogram(data_dist[bead], range=bin_range, bins=Nbins, weights=pd.DataFrame(W[value]))
                save[value] += c
        save[value] = save[value]/Nbeads      
        points = 0.5*(b[:-1]+b[1:])
        norm = 1/np.trapz(x = points, y=save[value])        
        save[value] = norm*save[value]
    
    return save, points
   
def calc_Block_SE(data, Natoms, n, ind, mode='scalar', bin_range = [0,200], Nbins = 200, beta=None, allbeads=None, dim=None, sign=None, Nbeads=0, dist=None, W_bias=None, label='d'):
    #Block average method for estimating error in average calculated from MD
    #Following Grossfield and Zuckerman Annu Rep Comput Chem 2009 5 23-48
    #This calculates the Block Standard Error (BSE)
    
    import numpy as np
    import matplotlib.pyplot as plt
    from auxfunctions_new import get_reweighted_hist, get_reweighted_hist_general_n
    
    N = len(data)

    save_M = np.zeros(len(n))
    BSE = np.zeros(len(n))
    BSE_hist = np.zeros((len(n), Nbins))

    for k in range(len(n)):
    
        #DIVIDE N DATA POINTS INTO M BLOCK OF LENGTH n[k]
        M = int(np.floor(N/n[k]))
                    
        av = np.zeros(M)
        hist = np.zeros((M,Nbins))
        hist_sq = np.zeros((M,Nbins))
        gethist={}
        #stdev = np.zeros(M)
        
        #LOOP OVER BLOCKS
        for j in range(M):
            #GET BLOCK DATA
            block_ind_0 = j*n[k]
            block_ind_l = ((j+1)*n[k])
            block = data.iloc[block_ind_0:block_ind_l]
            ind_block = ind.iloc[block_ind_0:block_ind_l]
            block_bias=None
            
            if W_bias is not None:
                block_bias = W_bias[block_ind_0:block_ind_l]
            
            block_dist = []
            for bead in range(Nbeads):
                #block_dist.append(dist[bead].iloc[block_ind_0:block_ind_l])
                block_dist.append(dist[bead][block_ind_0:block_ind_l])
                
            #IF THE QUANTITY IS SCALAR, JUST TAKE THE MEAN FOR EACH BLOCK
            if( mode == 'scalar'):
                if W_bias is not None:
                    av[j] = np.average(np.array(block), weights =  block_bias)
                else:
                    av[j] = np.average(np.array(block))
                    
            #IF YOU ARE INTERESTED IN THE RADIAL DIST FUNCTION, BUILD THE HIST FOR EACH BLOCK
            elif( mode == 'g_r' ):
                (hist[j,:], r_bins) = np.histogram(np.array(block), range=bin_range, bins=Nbins, normed = True)
            #IF YOU NEED TO REWEIGH BY Ws/Wf, IT'S A LITTLE TRICKIER, USE DEDICATED FUNCTION. DATA IS S IN THIS CASE,
            #allbeads HOLDS THE COORDINATES FOR ALL BEAD, NBEADS IS NUMBER OF BEADS
            elif( mode == 'g_r_PLUMED' ):
                (hist[j,:], bins, hist_sq[j,:]) = get_reweighted_hist(block_dist, block_bias, Nbeads, bin_range, Nbins, block,ind_block,label)
#                plt.figure(17)
#                plt.plot(bins, hist[j,:],'o')
            
            elif( mode == 'g_r_PLUMED_general_n' ):
                (hist[j,:], bins) = get_reweighted_hist_general_n(block_dist, block_bias, Nbeads, bin_range, Nbins, block, Natoms, label)
#                plt.figure(17)
#                plt.plot(bins, hist[j,:],'o')

            else:
                raise IOError('The analysis ' + mode + ' is not yet implemented')
    
        save_M[k] = M
        #IF SCALAR, THE BSE IS JUST THE STDEV BETWEEN AVERAGES OF DIFFERENT BLOCKS, DIVIDED BY # OF BLOCKS
        if( mode == 'scalar'):
            BSE[k] = av.std(ddof = 1)/np.sqrt(save_M[k])
        
        #IF RADIAL DIST FUNCTION, THE SAME, BUT FOR EACH POINT IN THE HISTOGRAM
        elif( mode == 'g_r' or mode == "g_r_PLUMED" or mode == "g_r_PLUMED_general_n" or mode == 'g_r_Wf' or mode == 'g_r_Wb' or mode == 'g_r_Wd'):
            BSE_hist[k,:] = hist.std(ddof = 1, axis=0)/np.sqrt(save_M[k])

        
#        elif( mode == 'g_r_all'):
#            BSE_hist_dict = {value: np.zeros((len(n), Nbins)) for value in sign}
#            for value in sign:
#                BSE_hist_dict[value][k,:] = hist_dict[value].std(axis=0)/np.sqrt(save_M[k])
            
            
    if( mode == 'scalar'):
        return BSE
        
    elif( mode == 'g_r' or  mode == "g_r_PLUMED" or  mode == "g_r_PLUMED_general_n" or mode == 'g_r_Wf' or mode == 'g_r_Wb' or mode == 'g_r_Wd'):
        return BSE_hist
    
    elif (mode == 'g_r_all' or mode == "g_r_all_PLUMED"):
        return BSE_hist_dict
        

def GetBSE(data, Natoms, index, minsize=1, maxsize=2100, step=100, Nrow=1, Ncol=1, Ncurrent=1, p0=[1E-6, 0.1, 1E-6], mode_s='scalar', range=[0,200], bins=200, doplot=False, b=None, beads=None, d=None, symsign=None, Nb=0, distance=None, bias_weights=None, l='d'):
    #Calls to calc_Block_SE above and plots to check convergence.

    import matplotlib.pyplot as plt
    import numpy as np
    from scipy import optimize
    
    #USE THIS TO DECIDE WHICH BLOCK SIZE TO USE
    block_length = np.arange(minsize, maxsize, step)
    
    #IF ALREADY DECIDED ON BLOCKSIZE, GIVE MINSIZE AND MAXSIZE AS EQUAL AND GET ONLY ONE VALUE OF BSE.
    #PROBLEM, YOU CAN'T FIT THIS THEN, SO THE ESTIMATE MAY BE NOISY IF THE BLOCK SIZE IS LARGE
    if (minsize == maxsize):
        block_length=[maxsize]
        
    BSE = calc_Block_SE(data, Natoms, block_length, ind=index, mode = mode_s, bin_range = range, Nbins= bins, beta=b, allbeads=beads, dim=d, sign=symsign, Nbeads=Nb, dist=distance, W_bias = bias_weights, label=l)
    
    if (doplot):
        plt.figure()    
       
        #print(params)

        plt.subplot(Nrow, Ncol, Ncurrent)
    
        if (mode_s == 'scalar'):
            plt.scatter(block_length, BSE, label='Data')
        elif( mode_s == 'g_r' or mode_s == "g_r_PLUMED"):
            plt.scatter(block_length, BSE[:,7], label='Data')
    
    #IF MINSIZE != MAXSIZE FIT THE RESULTS TO TANH AND GET AN ESTIMATE FOR BSE FROM IT
    if( mode_s == 'scalar' and minsize != maxsize ):
        def test_func(x, a, b):
            return a * np.tanh(b * x) 

        params, params_covariance = optimize.curve_fit(test_func, block_length, BSE, p0)
        asymptotic_BSE = params[0]
        if (doplot):
            plt.plot(block_length, test_func(block_length, params[0], params[1]),
                     label='Fitted function')
            plt.title('BSE is ' + '{:.4E}'.format(asymptotic_BSE))
            

    #IF MINSIZE == MAXSIZE DON'T FIT (THERE IS ONE BLOCK SIZE) SO USE THE ONE VALUE YOU HAVE            
    elif (mode_s == 'scalar' and minsize == maxsize ):
        asymptotic_BSE = BSE[0]    

    #FOR RADIAL DIST FUNCTION, ALWAYS JUST USE THE VALUE OF BSE YOU HAVE FROM LARGEST BLOCK
    elif( mode_s == 'g_r' or mode_s == "g_r_PLUMED" or mode_s == "g_r_PLUMED_general_n" or  mode_s == 'g_r_Wf' or mode_s == 'g_r_Wb' or mode_s == 'g_r_Wd'):
        asymptotic_BSE = BSE[-1][:]
    
#    elif( mode_s == 'g_r_all' or mode_s == 'g_r_all_PLUMED'):
#        asymptotic_BSE_dict = {value: np.zeros(bins) for value in symsign}
#        for value in symsign:
#            asymptotic_BSE_dict[value] = BSE[value][-1][:]
#            
            
    if (doplot):
        plt.grid(True)
        plt.xlabel('Block Size [Steps/Stride_print]')
        plt.ylabel('BSE')
        plt.ylim(0.0, 1.15*asymptotic_BSE)
        plt.legend(loc='best')  
    
#    if ( mode_s == 'g_r_all' or mode_s == "g_r_all_PLUMED"):
#        return asymptotic_BSE_dict
#    else:
    return asymptotic_BSE

def CalcPhiEstimator(allbeads, omega, Natoms, mode='Harm2D_iso',m=1):
    import numpy as np 
    
    if (mode == 'Harm1D'):
        k = m*omega**2
        sig = np.zeros(len(allbeads[0].x[:,0]))
        
        for n in range(Natoms):
            for bead in range(len(allbeads)):
                sig = sig + 0.5*k*(allbeads[bead].x[:,n])**2
            Phi=sig/len(allbeads)
    
    if (mode == 'Harm2D_iso'):
        k = m*omega**2
        sig = np.zeros(len(allbeads[0].x[:,0]))
        
        for n in range(Natoms):
            for bead in range(len(allbeads)):
                sig = sig + 0.5*k*((allbeads[bead].x[:,n])**2+(allbeads[bead].y[:,n])**2)
            Phi=sig/len(allbeads)
            
    if (mode == 'Harm3D_iso'):
        k = m*omega**2
        sig = np.zeros(len(allbeads[0].x[:,0]))
        
        for n in range(Natoms):
            for bead in range(len(allbeads)):
                sig = sig + 0.5*k*((allbeads[bead].x[:,n])**2+(allbeads[bead].y[:,n])**2+(allbeads[bead].z[:,n])**2)
            Phi=sig/len(allbeads)    
        
    return Phi

def CalcPhiEstimator_from_PLUMED(path, fname, Nbeads, step_start, step_end, skip, potlabel='PotEng'):
    import pandas as pd
    import numpy as np
    
    Phi = np.zeros(step_end-step_start)
    for n in range(Nbeads):
        fopen = path + fname + '.' + str(n)
        f = pd.read_csv(fopen, sep='\s+', skiprows=skip, nrows=step_end+1)
        f = f.iloc[step_start:step_end,:]
#        print(n)
#        print(len(f[potlabel]))
#        print(Nbeads)
        Phi = Phi + f[potlabel]
        
    return Phi/Nbeads


def CalcmodeT(path, fname, Nbeads, step_start, step_end, skip, potlabel):
    import pandas as pd
    import numpy as np
    itr = step_end - step_start
    modT = np.zeros(step_end - step_start)
    for n in range(Nbeads):
        fopen = path + fname + '.' + str(n)
        f = pd.read_csv(fopen, sep='\s+', skiprows=skip, nrows=step_end + 1)
        f = f.iloc[step_start:step_end, :]
        #        print(n)
        #        print(len(f[potlabel]))
        #        print(Nbeads)
        modT = modT + f[potlabel]

    return modT / Nbeads

def CalcVirEstimator_from_PLUMED(path, fname, Nbeads, step_start, step_end, skip, potlabel='v_newvir'):
    import pandas as pd
    import numpy as np
    
    res = np.zeros(step_end-step_start)
    for n in range(Nbeads):
        fopen = path + fname + '.' + str(n)
        f = pd.read_csv(fopen, sep='\s+', skiprows=skip, nrows=step_end+1)
        f = f.iloc[step_start:step_end,:]
#        print(n)
#        print(len(f[potlabel]))
#        print(Nbeads)
        res = res + f[potlabel]
        
    #VIR DOES NOT NEED THE DIVISION BY NBEADS SINCE THE FORCES ARE ALREADY DIVIDED BY NBEADS WITHIN LAMMPS! UNLIKE POT ENERGY
    return res

def CalcUintEstimator_from_PLUMED(path, fname, Nbeads, step_start, step_end, skip, potlabel='E_pair'):
    import pandas as pd
    import numpy as np
    
    res = np.zeros(step_end-step_start)
    for n in range(Nbeads):
        fopen = path + fname + '.' + '{:02d}'.format(n+1)
        f = pd.read_csv(fopen, sep='\s+', skiprows=skip, nrows=step_end+1)
        f = f.iloc[step_start:step_end,:]
#        print(n)
#        print(len(f[potlabel]))
#        print(Nbeads)
        res = res + f[potlabel]
        
    #VIR DOES NOT NEED THE DIVISION BY NBEADS SINCE THE FORCES ARE ALREADY DIVIDED BY NBEADS WITHIN LAMMPS! UNLIKE POT ENERGY
    return res/Nbeads

def CalcFnEstimator_from_PLUMED(path, fname, Nbeads, step_start, step_end, Natoms,m=1):
    import pandas as pd
    import numpy as np
    
    Phi = np.zeros(step_end-step_start)
    for n in range(Nbeads):
        if (n<100):
            fopen = path + fname + '_' + '{:02d}'.format(n+1) + '.xyz'
        else:
            raise("n>100")
        f = pd.read_csv(fopen, sep='\s+', nrows=Natoms*(step_end+1),header=None, comment='#')
        new_f = pd.DataFrame(f.values.reshape(-1,Natoms),columns=range(Natoms))

        new_f = new_f.iloc[step_start:step_end,:]
#        print(n)
#        print(len(f[potlabel]))
#        print(Nbeads)
        Phi = Phi + new_f.sum(axis=1)/m #sum over atoms and divide by mass
    
#    Phi = Phi/Nbeads
    return Phi

def CalcKEstimator(allbeads, omega, beta, Natoms, mode='Harm1D', m=1, d=1):
    import numpy as np 
    
    if (mode == 'Harm1D'):
        k = m*omega**2
        sig = np.zeros(len(allbeads[0].x[:,0]))
        
        #loop over atoms
        for n in range(Natoms):
            vir = np.zeros(len(allbeads[0].x[:,0]))
            xc = np.zeros(len(allbeads[0].x[:,0]))
            
            #calc the centroid
            for bead in range(len(allbeads)):
                xc = xc + allbeads[bead].x[:,n]
            xc=xc/len(allbeads)
            
            #sum over beads
            for bead in range(len(allbeads)):
                vir = k*allbeads[bead].x[:,n]
                sig = sig + vir*(allbeads[bead].x[:,n]-xc)
        
        K=0.5*sig/len(allbeads)+Natoms*d/(2*beta) 

    return K

def CalcKEstimator_wSym(allbeads, omega, beta, Natoms, Wsym, betas, mode='Harm1D', m=1, d=1):
    import numpy as np 
    
    if (mode == 'Harm1D' and Natoms ==2):
        k = m*omega**2
        sig = np.zeros(len(allbeads[0].x[:,0]))
        
        #Calc the centroid of both particles
        xc_tot = np.zeros(len(allbeads[0].x[:,0]))
        for n in range(Natoms):
            for bead in range(len(allbeads)):
                xc_tot = xc_tot + allbeads[bead].x[:,n]
        xc_tot=xc_tot/(len(allbeads)*Natoms)
        
        #loop over atoms
        for n in range(Natoms):
            vir = np.zeros(len(allbeads[0].x[:,0]))
            xc = np.zeros(len(allbeads[0].x[:,0]))
            
            #calc the centroid
            for bead in range(len(allbeads)):
                xc = xc + allbeads[bead].x[:,n]
            xc=xc/len(allbeads)
            
            #sum over beads
            for bead in range(len(allbeads)):
                vir = k*allbeads[bead].x[:,n]
                sig = sig + vir*(Wsym*allbeads[bead].x[:,n]-xc-np.exp(-betas)*xc_tot)
        
        K=0.5*sig/len(allbeads)+d/(2*beta)+d/(2*beta)*Wsym
    else:
        raise('IOError: CalcKEstimator_wSym does not work for more than 2 atoms now')

    return K

def Analyze_s_from_colvar(path, fname, Nbeads, tstart, tend, wall=True):
    
    import pandas as pd

    data = pd.read_csv(path + '/' + fname, sep='\s+', header=None, skiprows=1, usecols=[0,1])
    if(not wall):
        data.columns=['Time', 'CV']
    else:
        data.columns=['Time', 'CV']
    #Define range for analysis
    #data = data[data['Time'].between(tstart, tend)] 

    return data

def Analyze_s_from_PLUMED(path, fname, fname2, Nbeads, beta, plot=False):

    import numpy as np
    import matplotlib.pyplot as plt
    from auxfunctions import Analyze_s_from_colvar
    
    def read_s_hist(path, fname, Nbeads, tstart, tend):
    
        import pandas as pd

        data = pd.read_csv(path + '/' + fname, sep='\s+', header=None, skiprows=6, usecols=[0,1])
        data.columns=['CV', 'HIST']
    
        #Define range for analysis
        #data = data[data['Time'].between(tstart, tend)] 

        return data

    data2 = Analyze_s_from_colvar(path, fname2, Nbeads, tstart=0, tend=2000000)
    
    betas = data2['CV']
   
    #W(S) for bosons and fermions
    Wb_s = (1 + np.exp(- betas))
    Wf_s = (1 - np.exp(- betas))

    data = read_s_hist(path, fname, Nbeads, tstart=0, tend=2000000)
    betas_g = data['CV'].iloc[1:]
    p_s = data['HIST'].iloc[1:]
    
    Wb = (1 + np.exp(-betas_g))
    Wf = (1 - np.exp(-betas_g))
    
    #PLOT BETA * S
    plt.figure(1)
    plt.plot(betas)
    plt.grid(True)    
        
    #PLOT LOG[P(BETA*S)] VS. BETA*S
    plt.figure(2)
    tmp = -1/beta*np.log(p_s)
    plt.plot(betas_g,tmp-np.min(tmp), '-')
    plt.grid(True)
    plt.xlabel(r'$\beta s$')
    plt.ylabel(r'FES [kJ mol$^{-1}$ ]')
    #plt.xlim(-9,9)
    plt.show()
       
    plt.figure(3)
    plt.plot(betas_g, p_s*Wb , '-')
    plt.plot(betas_g, p_s*Wf , '-')
    plt.grid(True)
    plt.xlabel(r'$\beta s$')
    plt.ylabel(r'$p(s) \cdot W$')
    #plt.xlim(-9,9)
    plt.show()
    
    return betas, Wb_s, Wf_s, betas_g, p_s*Wb, p_s*Wf, p_s


def getBSE(data, w, step_start, step_end, columnname='Phi', Nblocks_list=[5]):

    for Nblocks in Nblocks_list:

        delta = int((step_end - step_start) / Nblocks)
        block_res = pd.DataFrame(index=range(Nblocks), columns=[columnname])
        block_res = block_res.fillna(np.nan)  # (0) # with 0s rather than NaNs
        init_step = step_start
        Wj = np.zeros(Nblocks)
        Aj = np.zeros(Nblocks)
        for block in range(Nblocks):
            block_data = data[init_step:(init_step + delta)]
            block_w = w[init_step:(init_step + delta)]
            init_step = init_step + delta

            block_res.loc[block, columnname] = np.average(block_data * block_w)
            #            Wj[block] = (np.sum(block_w))**2/np.sum(block_w**2)
            Wj[block] = np.sum(block_w)
            Aj[block] = np.sum(block_data * block_w) / np.sum(block_w)

        meanA = np.sum(Wj * Aj) / np.sum(Wj)
        V2 = np.sum(Wj ** 2)
        V1 = np.sum(Wj)
        #        BSEA = np.sqrt( np.sum( np.multiply(Wj,(Aj-meanA)**2) )/(V1 - V2/V1) )/np.sqrt(Nblocks)
        Neff = np.sum(Wj) ** 2 / np.sum(Wj ** 2)
        print("Nblocks is: " + str(Nblocks) + " and Neff is: " + str(Neff))
        # varA = np.sqrt( Neff*np.sum( save_data['Wj']*(save_data['EF']-meanA)**2 )/np.sum(save_data['Wj'])/np.sqrt(Neff-1) )
        varA = np.sqrt(
            Neff * np.sum(save_data['Wj'] * (save_data['EF'] - meanA) ** 2) / np.sum(save_data['Wj']) / np.sqrt(Neff))
        BSEA = varA / np.sqrt(Nblocks)  # /np.sqrt(Nblocks)

    #        BSEA = np.sqrt( np.sum( np.multiply(Wj,(Aj-meanA)**2) )/V1 )/np.sqrt(Nblocks)
    #        mean = block_res[columnname].mean()
    #        BSE =  block_res[columnname].std(ddof=1)/np.sqrt(Nblocks)

    return (meanA, BSEA)


def calc_Wn(beta, data, n, save_label_e, label='B'):
    import numpy as np
    ################################
    # PRINT Wn FOR BOSON OR FERMIONS#
    ################################
    ib = 1 / beta

    # Define function string. If I am running Bosons I am already calculating dlnWB/dbeta
    if label == 'B':
        save_label_n_new = ["+data['VB" + str(x) + "']" for x in range(1, n)]
        save_label_n_new = [''] + save_label_n_new
        sign = [1.0 for k in range(1, n + 1)]
        func_string = [str(z) + "*np.exp(-beta*(data['E" + x + "']" + y + "))" for x, y, z in
                       zip(save_label_e, save_label_n_new, sign)]
        res = eval("1/n*(" + "+".join(func_string) + ")")

    elif label == 'F':
        save_label_n_new = ["*data_F['WF" + str(x) + "']" for x in range(1, n)]
        save_label_n_new = [''] + save_label_n_new
        sign = [pow(-1.0, k - 1) for k in range(n, 0, -1)]
        func_string = [str(z) + "*np.exp(-beta*(data['E" + x + "']))" + y for x, y, z in
                       zip(save_label_e, save_label_n_new, sign)]
        res = eval("1/n*(" + "+".join(func_string) + ")")
    else:
        raise ('IOError: Please use labels F or B')

    #    res = eval("data['E"+ save_label_e[-1] + "'] - ib*np.log(1/" + str(n) + "*(" + "+".join(func_string) + "))")
    return res


def permutation_prob_3(filename, beta, cut, perm_length):
    import pandas as pd
    import numpy as np
    '''
    :param filename: pimdb.log file
    :param beta: beta in 1 / kJ/mol
    :param cut: he first "cut" values will be eliminated.
    :return: length of array l and permutation praobability
    '''
    # Here the pimdb.log file is in units of kJ/mol hence beta has to also be in same units
    df = pd.read_csv(filename, delimiter='\s+')
    df = df.iloc[cut:-1] # I added this and removed "cut from all [] example  e_1_1 = df.iloc[cut:, 0]

    v_3_column = df.iloc[:, -1]
    v_2_column = df.iloc[:, -2]
    v_1_column = df.iloc[:, -3]
    v_0_column = df.iloc[:, -4]
    # e_1_1 = df.iloc[:, 0]
    # e_2_2 = df.iloc[:, 1]
    # e_2_1 = df.iloc[:, 2]
    e_3_3 = df.iloc[:, 3]
    e_3_2 = df.iloc[:, 4]
    e_3_1 = df.iloc[:, 5]

    v_3 = np.array([v_2_column, v_1_column, v_0_column])
    e_3 = np.array([e_3_1, e_3_2, e_3_3])
    permutation_probability = np.array([])
    length_array = len(e_3[0])
    p_l_denom = np.exp(- beta * (e_3_1 + v_2_column)) + np.exp(- beta * (e_3_2 + v_1_column)) \
                + np.exp(- beta * (e_3_3 + v_0_column))
    for j in range(0, perm_length):
        p_l_num = np.exp(- beta * (e_3[j] + v_3[j]))
        p_l = np.asarray(p_l_num / p_l_denom)
        # permutation_probability = np.append(permutation_probability, np.array(p_l))      #THIS
        permutation_probability = np.append(permutation_probability, np.mean(np.array(p_l)))

    l_array = np.arange(1, perm_length+1)  # array([1, 2, 3])

    # return l_array, permutation_probability.reshape((perm_length), length_array)         #THIS
    return l_array, permutation_probability


def permutation_prob_10(filename, beta, cut, perm_length):
    '''
    :param filename: pimdb.log file
    :param beta: beta in 1 / kJ/mol
    :param cut: he first "cut" values will be eliminated.
    :return: length of array l and permutation praobability
    '''
    # Here the pimdb.log file is in units of kJ/mol hence beta has to also be in same units
    df = pd.read_csv(filename, delimiter='\s+')
    df = df.iloc[cut:-1] # I added this and removed "cut from all [] example  e_1_1 = df.iloc[cut:, 0]

    v_10_column = df.iloc[:, -1]
    v_9_column = df.iloc[:, -2]
    v_8_column = df.iloc[:, -3]
    v_7_column = df.iloc[:, -4]
    v_6_column = df.iloc[:, -5]
    v_5_column = df.iloc[:, -6]
    v_4_column = df.iloc[:, -7]
    v_3_column = df.iloc[:, -8]
    v_2_column = df.iloc[:, -9]
    v_1_column = df.iloc[:, -10]
    v_0_column = df.iloc[:, -11]


    e_10_1 = df.iloc[:, -12]
    e_10_2 = df.iloc[:, -13]
    e_10_3 = df.iloc[:, -14]
    e_10_4 = df.iloc[:, -15]
    e_10_5 = df.iloc[:, -16]
    e_10_6 = df.iloc[:, -17]
    e_10_7 = df.iloc[:, -18]
    e_10_8 = df.iloc[:, -19]
    e_10_9 = df.iloc[:, -20]
    e_10_10 = df.iloc[:, -21]

    v_10 = np.array([v_9_column, v_8_column, v_7_column, v_6_column, v_5_column,
                     v_4_column, v_3_column, v_2_column, v_1_column, v_0_column])
    e_10 = np.array([e_10_1, e_10_2, e_10_3, e_10_4, e_10_5, e_10_6, e_10_7, e_10_8, e_10_9, e_10_10])

    permutation_probability = np.array([])
    length_array = len(e_10[0])
    p_l_denom = np.exp(- beta * (e_10_1 + v_9_column)) + np.exp(- beta * (e_10_2 + v_8_column)) + \
                np.exp(- beta * (e_10_3 + v_7_column)) + np.exp(- beta * (e_10_4 + v_6_column)) + \
                np.exp(- beta * (e_10_5 + v_5_column)) + np.exp(- beta * (e_10_6 + v_4_column)) + \
                np.exp(- beta * (e_10_7 + v_3_column)) + np.exp(- beta * (e_10_8 + v_2_column)) + \
                np.exp(- beta * (e_10_9 + v_1_column)) + np.exp(- beta * (e_10_10 + v_0_column))
    for j in range(0, perm_length):
        p_l_num = np.exp(- beta * (e_10[j] + v_10[j]))
        p_l = np.asarray(p_l_num / p_l_denom)
        # permutation_probability = np.append(permutation_probability, np.array(p_l))      #THIS
        permutation_probability = np.append(permutation_probability, np.mean(np.array(p_l)))

    l_array = np.arange(1, perm_length+1)  # array([1, 2, 3])

    # return l_array, permutation_probability.reshape((perm_length), length_array)          #THIS
    return l_array, permutation_probability

